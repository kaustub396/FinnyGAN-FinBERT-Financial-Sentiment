{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20c61ebd-11cd-4c6b-bd93-e486085dcc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded FinBERT model and tokenizer from: E:/Research-8177/finbert_fiqa_final\n",
      "Step 1: Extracting embeddings from real data...\n",
      "Step 2: Training GAN to generate synthetic embeddings...\n",
      "Starting GAN training...\n",
      "Epoch 10/50 | Loss D: 0.1414 | Loss G: 5.5034\n",
      "Epoch 20/50 | Loss D: 0.0055 | Loss G: 4.3198\n",
      "Epoch 30/50 | Loss D: 0.9582 | Loss G: 6.3785\n",
      "Epoch 40/50 | Loss D: 0.0817 | Loss G: 3.7977\n",
      "Epoch 50/50 | Loss D: 0.9463 | Loss G: 3.6760\n",
      "GAN training complete.\n",
      "Step 3: Generating synthetic embeddings...\n",
      "Generated 2000 synthetic embeddings.\n",
      "Step 4: Preparing data for fine-tuning...\n",
      "Total real training samples: 4673\n",
      "Total synthetic samples: 2000\n",
      "Step 5: Fine-tuning FinBERT on the augmented data...\n",
      "Starting fine-tuning...\n",
      "Epoch 1/3, Average Loss: 0.8504\n",
      "Epoch 2/3, Average Loss: 0.5605\n",
      "Epoch 3/3, Average Loss: 0.4872\n",
      "Step 6: Saving the fine-tuned model...\n",
      " Final model saved at E:/Research-8177/finbert_fiqa_final\n",
      "\n",
      "--- Starting Evaluation on Test Set ---\n",
      "\n",
      "Evaluation Results on Test Set:\n",
      "  Accuracy: 0.7613\n",
      "  Precision: 0.7541\n",
      "  Recall: 0.7613\n",
      "  F1 Score: 0.7572\n",
      "\n",
      "Training and evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# PATHS and HYPERPARAMETERS =====\n",
    "TRAIN_FILE = r\"E:\\Research-8177\\fiqa_train.csv\"\n",
    "TEST_FILE  =r\"E:\\Research-8177\\fiqa_test.csv\"\n",
    "\n",
    "MODEL_NAME = \"yiyanghkust/finbert-tone\"\n",
    "SAVE_DIR = \"E:/Research-8177/finbert_fiqa_final\"\n",
    "\n",
    "\n",
    "# Hyperparameters for the fine-tuning process\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# GAN hyperparameters\n",
    "GAN_EPOCHS = 50\n",
    "NOISE_DIM = 100\n",
    "EMBED_DIM = 768\n",
    "SYNTHETIC_SAMPLES = 2000\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# DATASET CLASSES =====\n",
    "class FiqaDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_len=128):\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        self.texts = self.df[\"text\"].astype(str).tolist()\n",
    "        self.labels = self.df[\"label\"].astype(int).tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class SyntheticDataset(Dataset):\n",
    "    def __init__(self, synthetic_embeddings, num_labels=3):\n",
    "        self.synthetic_embeddings = synthetic_embeddings\n",
    "        self.synthetic_labels = torch.randint(0, num_labels, (len(synthetic_embeddings),), dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.synthetic_embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"embeddings\": self.synthetic_embeddings[idx],\n",
    "            \"labels\": self.synthetic_labels[idx]\n",
    "        }\n",
    "\n",
    "# LOAD FINBERT & TOKENIZER =====\n",
    "try:\n",
    "    tokenizer = BertTokenizer.from_pretrained(SAVE_DIR)\n",
    "    finbert = BertForSequenceClassification.from_pretrained(SAVE_DIR)\n",
    "    print(f\"Loaded FinBERT model and tokenizer from: {SAVE_DIR}\")\n",
    "except OSError:\n",
    "    print(f\"Model not found at {SAVE_DIR}. Loading base model from {MODEL_NAME}.\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "    config = BertConfig.from_pretrained(MODEL_NAME, num_labels=3)\n",
    "    finbert = BertForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "\n",
    "finbert.to(device)\n",
    "\n",
    "#  GAN MODEL CLASSES =====\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim=NOISE_DIM, embed_dim=EMBED_DIM):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, embed_dim),\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embed_dim=EMBED_DIM):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "#GAN TRAINING FUNCTIONS =====\n",
    "def get_embeddings(texts, model, tokenizer, batch_size=16):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            enc = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=128).to(device)\n",
    "            outputs = model.bert(**enc)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            embeddings.append(cls_embeddings.detach().cpu())\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "def train_gan(real_embeddings, epochs=GAN_EPOCHS, batch_size=64, noise_dim=NOISE_DIM):\n",
    "    generator = Generator(noise_dim=noise_dim).to(device)\n",
    "    discriminator = Discriminator().to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optim_G = optim.Adam(generator.parameters(), lr=2e-4)\n",
    "    optim_D = optim.Adam(discriminator.parameters(), lr=2e-4)\n",
    "\n",
    "    dataset = torch.utils.data.DataLoader(real_embeddings, batch_size=batch_size, shuffle=True)\n",
    "    print(\"Starting GAN training...\")\n",
    "    for epoch in range(epochs):\n",
    "        for real in dataset:\n",
    "            real = real.to(device)\n",
    "            current_batch_size = real.size(0)\n",
    "            real_labels = torch.ones(current_batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(current_batch_size, 1).to(device)\n",
    "\n",
    "            # Train Discriminator ---\n",
    "            z = torch.randn(current_batch_size, noise_dim).to(device)\n",
    "            fake = generator(z)\n",
    "            d_real = discriminator(real)\n",
    "            d_fake = discriminator(fake.detach())\n",
    "            loss_D = criterion(d_real, real_labels) + criterion(d_fake, fake_labels)\n",
    "            optim_D.zero_grad()\n",
    "            loss_D.backward()\n",
    "            optim_D.step()\n",
    "\n",
    "            #Train Generator ---\n",
    "            z = torch.randn(current_batch_size, noise_dim).to(device)\n",
    "            fake = generator(z)\n",
    "            d_fake = discriminator(fake)\n",
    "            loss_G = criterion(d_fake, real_labels)\n",
    "            optim_G.zero_grad()\n",
    "            loss_G.backward()\n",
    "            optim_G.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Loss D: {loss_D.item():.4f} | Loss G: {loss_G.item():.4f}\")\n",
    "    \n",
    "    print(\"GAN training complete.\")\n",
    "    return generator\n",
    "\n",
    "def generate_synthetic_embeddings(generator, n_samples=SYNTHETIC_SAMPLES, noise_dim=NOISE_DIM):\n",
    "    generator.eval()\n",
    "    z = torch.randn(n_samples, noise_dim).to(device)\n",
    "    with torch.no_grad():\n",
    "        fake_embeddings = generator(z)\n",
    "    return fake_embeddings.cpu()\n",
    "\n",
    "def train_model(model, real_loader, synthetic_loader, epochs, lr):\n",
    "    model.train()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    real_iter = iter(real_loader)\n",
    "    synthetic_iter = iter(synthetic_loader)\n",
    "\n",
    "    print(\"Starting fine-tuning...\")\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        # Alternate between real and synthetic data batches\n",
    "        while True:\n",
    "            real_batch = next(real_iter, None)\n",
    "            if real_batch is not None:\n",
    "                # Train on real data\n",
    "                inputs = {k: v.to(device) for k, v in real_batch.items()}\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(**inputs)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "            \n",
    "            synthetic_batch = next(synthetic_iter, None)\n",
    "            if synthetic_batch is not None:\n",
    "                # Train on synthetic data\n",
    "                embeddings = synthetic_batch[\"embeddings\"].to(device)\n",
    "                labels = synthetic_batch[\"labels\"].to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Get the classifier head\n",
    "                classifier = model.classifier\n",
    "                # Forward pass through classifier\n",
    "                logits = classifier(embeddings)\n",
    "                \n",
    "                loss = loss_fn(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "            if real_batch is None and synthetic_batch is None:\n",
    "                break\n",
    "        \n",
    "        # Reset iterators for next epoch\n",
    "        real_iter = iter(real_loader)\n",
    "        synthetic_iter = iter(synthetic_loader)\n",
    "\n",
    "        avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "#  MAIN PIPELINE EXECUTION =====\n",
    "if __name__ == \"__main__\":\n",
    "    df_train = pd.read_csv(TRAIN_FILE)\n",
    "    fiqa_texts = df_train[\"text\"].astype(str).tolist()\n",
    "    fiqa_labels = df_train[\"label\"].astype(int).tolist()\n",
    "\n",
    "    # GAN DATA AUGMENTATION ---\n",
    "    print(\"Step 1: Extracting embeddings from real data...\")\n",
    "    real_embeddings = get_embeddings(fiqa_texts, finbert, tokenizer)\n",
    "\n",
    "    print(\"Step 2: Training GAN to generate synthetic embeddings...\")\n",
    "    generator = train_gan(real_embeddings, epochs=GAN_EPOCHS)\n",
    "    \n",
    "    print(\"Step 3: Generating synthetic embeddings...\")\n",
    "    synthetic_embeddings = generate_synthetic_embeddings(generator, n_samples=SYNTHETIC_SAMPLES)\n",
    "    print(f\"Generated {len(synthetic_embeddings)} synthetic embeddings.\")\n",
    "\n",
    "    #FINETUNING WITH AUGMENTED DATA ---\n",
    "    print(\"Step 4: Preparing data for fine-tuning...\")\n",
    "    real_dataset = FiqaDataset(TRAIN_FILE, tokenizer)\n",
    "    synthetic_dataset = SyntheticDataset(synthetic_embeddings)\n",
    "    test_dataset = FiqaDataset(TEST_FILE, tokenizer)\n",
    "\n",
    "    real_loader = DataLoader(real_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    synthetic_loader = DataLoader(synthetic_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    print(f\"Total real training samples: {len(real_dataset)}\")\n",
    "    print(f\"Total synthetic samples: {len(synthetic_dataset)}\")\n",
    "\n",
    "    print(\"Step 5: Fine-tuning FinBERT on the augmented data...\")\n",
    "    train_model(finbert, real_loader, synthetic_loader, EPOCHS, LR)\n",
    "\n",
    "    #SAVE FINAL MODEL =====\n",
    "    print(\"Step 6: Saving the fine-tuned model...\")\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    finbert.save_pretrained(SAVE_DIR)\n",
    "    tokenizer.save_pretrained(SAVE_DIR)\n",
    "    print(f\" Final model saved at {SAVE_DIR}\")\n",
    "\n",
    "    #EVALUATION =====\n",
    "    print(\"\\n--- Starting Evaluation on Test Set ---\")\n",
    "    finbert.eval()\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = finbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"\\nEvaluation Results on Test Set:\")\n",
    "    print(f\"  Accuracy: {acc:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1 Score: {f1:.4f}\")\n",
    "\n",
    "    print(\"\\nTraining and evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6816f-799c-425d-bf5f-99c1251fb78a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
